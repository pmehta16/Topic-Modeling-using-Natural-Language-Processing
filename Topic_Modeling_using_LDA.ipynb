{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b17b75",
   "metadata": {},
   "source": [
    "<b> Step 1 : Load the Dataset </b>\n",
    "    \n",
    "Dataset: https://www.kaggle.com/datasets/akash14/news-category-dataset?select=Data_Train.csv\n",
    "\n",
    " About Dataset: Size of training set: 7,628 records Size of test set: 2,748 records. FEATURES: STORY: A part of the main content of the article to be published as a piece of news. SECTION: The genre/category the STORY falls in. There are four distinct sections where each story may fall in to. \n",
    "The Sections are labelled as follows : Politics: 0 Technology: 1 Entertainment: 2 Business: 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133fa279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORY</th>\n",
       "      <th>SECTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But the most painful was the huge reversal in ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How formidable is the opposition alliance amon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Most Asian currencies were trading lower today...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you want to answer any question, click on ‘...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In global markets, gold prices edged up today ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623</th>\n",
       "      <td>Karnataka has been a Congress bastion, but it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7624</th>\n",
       "      <td>The film, which also features Janhvi Kapoor, w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7625</th>\n",
       "      <td>The database has been created after bringing t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7626</th>\n",
       "      <td>The state, which has had an uneasy relationshi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7627</th>\n",
       "      <td>Virus stars Kunchacko Boban, Tovino Thomas, In...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7628 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  STORY  SECTION\n",
       "0     But the most painful was the huge reversal in ...        3\n",
       "1     How formidable is the opposition alliance amon...        0\n",
       "2     Most Asian currencies were trading lower today...        3\n",
       "3     If you want to answer any question, click on ‘...        1\n",
       "4     In global markets, gold prices edged up today ...        3\n",
       "...                                                 ...      ...\n",
       "7623  Karnataka has been a Congress bastion, but it ...        0\n",
       "7624  The film, which also features Janhvi Kapoor, w...        2\n",
       "7625  The database has been created after bringing t...        1\n",
       "7626  The state, which has had an uneasy relationshi...        0\n",
       "7627  Virus stars Kunchacko Boban, Tovino Thomas, In...        2\n",
       "\n",
       "[7628 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "train_data_df = pd.read_csv(\"Data_train.csv\", header = 0,encoding='cp1252') \n",
    "train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b8e23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019 will see gadgets like gaming smartphones ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It has also unleashed a wave of changes in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It can be confusing to pick the right smartpho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The mobile application is integrated with a da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We have rounded up some of the gadgets that sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>According to researchers, fraud in the mobile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>The iPhone XS and XS Max share the Apple A12 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>On the photography front, the Note 5 Pro featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>UDAY mandated that discoms bring the gap betwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>Ripple also helps bank customers send money to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2748 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  STORY\n",
       "0     2019 will see gadgets like gaming smartphones ...\n",
       "1     It has also unleashed a wave of changes in the...\n",
       "2     It can be confusing to pick the right smartpho...\n",
       "3     The mobile application is integrated with a da...\n",
       "4     We have rounded up some of the gadgets that sh...\n",
       "...                                                 ...\n",
       "2743  According to researchers, fraud in the mobile ...\n",
       "2744  The iPhone XS and XS Max share the Apple A12 c...\n",
       "2745  On the photography front, the Note 5 Pro featu...\n",
       "2746  UDAY mandated that discoms bring the gap betwe...\n",
       "2747  Ripple also helps bank customers send money to...\n",
       "\n",
       "[2748 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df = pd.read_csv(\"Data_test.csv\", header = 0,encoding='cp1252') \n",
    "test_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae3d6bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7628, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad85d98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“The whole thing feels like a giant set, stately and ponderous and minus impact; the cast all costumed and perfumed and largely lifeless, sparking only in bits and pieces,” a section of her review reads\\n\\n\\n On a star based rating, I would give this movie a blackhole\"Kalank mints money overseasTrade analyst Taran Adarsh shared Kalank\\'s overseas performance\\n\\n\\n05 mn\\nAustralia: A$ 620k\"Sonakshi Sinha on failure: I don\\'t lose hopeSonakshi Sinha has been having a bad run at the box office with her past few films including Kalank failing to impress the audience'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Previewing a random story from train data.\n",
    "'''\n",
    "train_data_df['STORY'][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b48a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683d787",
   "metadata": {},
   "source": [
    "<b> Step 2 : Data Preprocessing</b>\n",
    "```\n",
    "1.) Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "2.) Words that have fewer than 3 characters are removed.\n",
    "3.) All stopwords are removed.\n",
    "4.) Words are lemmatized - words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "5.) Words are stemmed - words are reduced to their root form.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60315a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Gensim and nltk libraries\n",
    "'''\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f37f795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pranjalimehta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e5b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdde2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to perform the pre processing steps on the entire dataset.\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "656a9ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Story: \n",
      "['Apart', 'from', 'Aniston', 'and', 'Sandler,', 'Murder', 'Mystery', 'also', 'features', 'Luke', 'Evans,', 'Gemma', 'Arterton,', 'John', 'Kani,', 'Ólafur', 'Darri', 'Ólafsson', 'and', 'Terrence', 'Stamp', 'among', 'others', 'in', 'pivotal', 'roles']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized Story: \n",
      "['apart', 'aniston', 'sandler', 'murder', 'mysteri', 'featur', 'luke', 'evan', 'gemma', 'arterton', 'john', 'kani', 'ólafur', 'darri', 'ólafsson', 'terrenc', 'stamp', 'pivot', 'role']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Previewing a random story from train data after preprocessing.\n",
    "'''\n",
    "story_num = random.randint(0,train_data_df.shape[0])\n",
    "story_sample = train_data_df['STORY'][story_num]\n",
    "print(\"Original Story: \")\n",
    "words = []\n",
    "for word in story_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized Story: \")\n",
    "print(preprocess(story_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6381cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_stories = []\n",
    "\n",
    "for story in train_data_df['STORY']:\n",
    "    processed_stories.append(preprocess(story))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31c67b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pain', 'huge', 'revers', 'incom', 'unheard', 'privat', 'sector', 'lender', 'essenti', 'mean', 'bank', 'take', 'grant', 'fee', 'structur', 'loan', 'deal', 'pay', 'account', 'upfront', 'book', 'borrow', 'turn', 'default', 'fee', 'tie', 'loan', 'deal', 'fell', 'crack', 'gill', 'vow', 'shift', 'safer', 'account', 'practic', 'amort', 'incom', 'book', 'upfront', 'gill', 'mend', 'past', 'way', 'mean', 'nasti', 'surpris', 'futur', 'good', 'news', 'consid', 'investor', 'love', 'clean', 'imag', 'loath', 'uncertainti', 'gain', 'pain', 'promis', 'strong', 'stabl', 'balanc', 'sheet', 'come', 'sacrific', 'investor', 'hop', 'phenomen', 'growth', 'promis', 'kapoor'], ['formid', 'opposit', 'allianc', 'congress', 'jharkhand', 'mukti', 'morcha', 'jharkhand', 'vika', 'morcha', 'prajatantrik']]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview 'processed_stories'\n",
    "'''\n",
    "print(processed_stories[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2079a7c1",
   "metadata": {},
   "source": [
    "<b> Step 3 : Bag of Words on the dataset</b>\n",
    "<br>\n",
    "I created a dictionary from 'processed_stories' containing the number of times a word appears in the training set. I have used \n",
    "```genism.coropa.Dictionary()``` for this. Followed by some more filtering out of data.\n",
    "\n",
    "Steps Involved: \n",
    "\n",
    "1. Create dictionary from words present in the entire training data.\n",
    "2. Remove very rare and very common words from the dictionary under consideration.\n",
    "3. Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3e37b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1.) Create a dictionary from 'processed_stories' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50d1cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2.) Removing very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cce8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each story we create a dictionary reporting how many\n",
    "words and how many times those words appear. Saved this to 'bow_corpus'.\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_stories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e13f529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 13 (\"fell\") appears 3 time.\n",
      "Word 15 (\"gain\") appears 2 time.\n",
      "Word 57 (\"currenc\") appears 1 time.\n",
      "Word 58 (\"dollar\") appears 2 time.\n",
      "Word 59 (\"index\") appears 6 time.\n",
      "Word 61 (\"japanes\") appears 2 time.\n",
      "Word 72 (\"south\") appears 1 time.\n",
      "Word 76 (\"trade\") appears 1 time.\n",
      "Word 105 (\"equiti\") appears 1 time.\n",
      "Word 112 (\"rise\") appears 1 time.\n",
      "Word 114 (\"spot\") appears 1 time.\n",
      "Word 145 (\"world\") appears 1 time.\n",
      "Word 235 (\"korea\") appears 1 time.\n",
      "Word 284 (\"expect\") appears 1 time.\n",
      "Word 433 (\"increas\") appears 1 time.\n",
      "Word 473 (\"emerg\") appears 2 time.\n",
      "Word 605 (\"countri\") appears 1 time.\n",
      "Word 619 (\"nation\") appears 1 time.\n",
      "Word 660 (\"give\") appears 1 time.\n",
      "Word 730 (\"highest\") appears 3 time.\n",
      "Word 741 (\"boost\") appears 1 time.\n",
      "Word 781 (\"asia\") appears 1 time.\n",
      "Word 846 (\"region\") appears 1 time.\n",
      "Word 954 (\"week\") appears 4 time.\n",
      "Word 962 (\"drop\") appears 1 time.\n",
      "Word 972 (\"surg\") appears 1 time.\n",
      "Word 1010 (\"msci\") appears 3 time.\n",
      "Word 1100 (\"wider\") appears 1 time.\n",
      "Word 1131 (\"pace\") appears 1 time.\n",
      "Word 1242 (\"bloomberg\") appears 1 time.\n",
      "Word 1606 (\"york\") appears 1 time.\n",
      "Word 1747 (\"deficit\") appears 1 time.\n",
      "Word 1994 (\"hong\") appears 1 time.\n",
      "Word 1995 (\"kong\") appears 1 time.\n",
      "Word 2077 (\"jump\") appears 1 time.\n",
      "Word 2351 (\"euro\") appears 1 time.\n",
      "Word 2531 (\"europ\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed stories\n",
    "'''\n",
    "random_story_num = random.randint(0,train_data_df.shape[0])\n",
    "bow_doc_x = bow_corpus[random_story_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177481f",
   "metadata": {},
   "source": [
    "<b> Step 4 : Running LDA using Bag of Words</b>\n",
    "<br>\n",
    "I will be training my lda model using ```gensim.models.LdaMulticore ```  Some of the parameters which I have tried to tweak are:\n",
    "\n",
    "1. num_topics: is the number of requested latent topics to be extracted from the training corpus.\n",
    "2. id2word: is a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "3. workers: is the number of extra processes to use for parallelization. Uses all available cores by default.\n",
    "4. alpha and eta: Hyperparameters affecting the sparsity of stories.\n",
    "5. passes: No of training passes through the corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1190c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training the lda model using gensim.models.LdaMulticore and saving it to 'lda_model'\n",
    "For my use case I will be chosing num_topics = 4.\n",
    "'''\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 4, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0491866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.013*\"film\" + 0.006*\"bank\" + 0.006*\"trade\" + 0.006*\"growth\" + 0.005*\"actor\" + 0.005*\"month\" + 0.005*\"crore\" + 0.005*\"quarter\" + 0.005*\"investor\" + 0.005*\"stock\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.023*\"seat\" + 0.015*\"poll\" + 0.014*\"sabha\" + 0.013*\"vote\" + 0.010*\"contest\" + 0.010*\"candid\" + 0.009*\"constitu\" + 0.009*\"minist\" + 0.009*\"leader\" + 0.008*\"phase\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.014*\"smartphon\" + 0.011*\"appl\" + 0.010*\"phone\" + 0.009*\"camera\" + 0.009*\"devic\" + 0.008*\"featur\" + 0.007*\"samsung\" + 0.007*\"googl\" + 0.007*\"launch\" + 0.007*\"iphon\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.011*\"govern\" + 0.010*\"modi\" + 0.006*\"minist\" + 0.006*\"issu\" + 0.006*\"data\" + 0.005*\"secur\" + 0.005*\"polit\" + 0.005*\"nation\" + 0.005*\"countri\" + 0.005*\"facebook\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we can see the words occuring in that topic and their relative weights.\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d4bb3",
   "metadata": {},
   "source": [
    "<b> Classification of Topics</b>\n",
    "<br>\n",
    "\n",
    "From the above results we can categorize the Topics as follows: \n",
    "* Topic 0: Business\n",
    "* Topic 1: Politics\n",
    "* Topic 2: Technology\n",
    "* Topic 3: Entertainment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd39fb5",
   "metadata": {},
   "source": [
    "<b> Step 6 : Testing the mode</b>\n",
    "<br>\n",
    "WIP.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
